



<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="૮₍ ˃ ⤙ ˂ ₎ა " href="https://ayanamisaki.github.io/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="૮₍ ˃ ⤙ ˂ ₎ა " href="https://ayanamisaki.github.io/atom.xml" />
<link rel="alternate" type="application/json" title="૮₍ ˃ ⤙ ˂ ₎ა " href="https://ayanamisaki.github.io/feed.json" />

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  

<link rel="canonical" href="https://ayanamisaki.github.io/2022/03/18/NeRF%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-NeRF%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields%20for%20View%20Synthesis/">



  <title>
NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis |
ayanamisaki = ૮₍ ˃ ⤙ ˂ ₎ა </title>
<meta name="generator" content="Hexo 5.4.1"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis
  </h1>
  
<div class="meta">
  <span class="item" title="创建时间：2022-03-18 15:37:44">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">发表于</span>
    <time itemprop="dateCreated datePublished" datetime="2022-03-18T15:37:44+08:00">2022-03-18</time>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="切换导航栏">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">ayanamisaki</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giph4lm9i7j20zk0m84qp.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicitspjpbj20zk0m81ky.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giph4wqtg4j20zk0m8x6p.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclxp31goj20zk0m8qv5.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipevo9j1jj20zk0m8e81.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclflwv2aj20zk0m84qp.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">首页</a></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN">
  <link itemprop="mainEntityOfPage" href="https://ayanamisaki.github.io/2022/03/18/NeRF%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-NeRF%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields%20for%20View%20Synthesis/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/avatar.jpg">
    <meta itemprop="name" content="ayanamisaki">
    <meta itemprop="description" content=", ">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="૮₍ ˃ ⤙ ˂ ₎ა ">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <h1 id="概述"><a class="markdownIt-Anchor" href="#概述">#</a> 概述</h1>
<h2 id="基本信息"><a class="markdownIt-Anchor" href="#基本信息">#</a> 基本信息</h2>
<ul>
<li>标题：NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
<ul>
<li>《NeRF: 将场景表示为用于视图合成的神经辐射场》</li>
</ul>
</li>
<li>作者：
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ibWlsZC5naXRodWIuaW8v">Ben Mildenhall*</span>（UC Berkeley）</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9wcmF0dWxzcmluaXZhc2FuLmdpdGh1Yi5pby8=">Pratul P. Srinivasan*</span>（UC Berkeley）</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWF0dGhld3RhbmNpay5jb20v">Matthew Tancik*</span>（UC Berkeley）</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9qb25iYXJyb24uaW5mby8=">Jonathan T. Barron</span>（Google Research）</li>
<li><span class="exturl" data-url="aHR0cDovL2NzZXdlYi51Y3NkLmVkdS9+cmF2aXIv">Ravi Ramamoorthi</span>（UC San Diego）</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cyLmVlY3MuYmVya2VsZXkuZWR1L0ZhY3VsdHkvSG9tZXBhZ2VzL3lpcmVubmcuaHRtbA==">Ren Ng</span>（UC Berkeley）</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JtaWxkL25lcmY=">GitHub 仓库</span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWF0dGhld3RhbmNpay5jb20vbmVyZg==">官网</span></li>
</ul>
<h2 id="核心"><a class="markdownIt-Anchor" href="#核心">#</a> 核心</h2>
<p><strong>非显式地将一个复杂的静态场景用一个神经网络来建模，</strong></p>
<ul>
<li>
<p><strong>在网络训练完成后，可以从任意角度渲染出清晰的场景图片</strong></p>
</li>
<li>
<p><strong>用一个 MLP 神经网络去隐式地学习一个静态 3D 场景</strong></p>
</li>
<li>
<p>基于训练好的神经网络，即可以从任意角度渲染出图片结果了</p>
</li>
<li>
<p>训练 MLP 的条件：针对一个静态场景，需要提供大量 **【相机参数已知】** 的图片。</p>
</li>
</ul>
<h2 id="理解重点"><a class="markdownIt-Anchor" href="#理解重点">#</a> 理解重点</h2>
<p><strong>理解 NeRF 是如何从一系列 2D 图像中学习到 3D 场景，又是如何渲染出 2D 图像</strong></p>
<ul>
<li>如何用 NeRF 来表示 3D 场景？</li>
<li>如何基于 NeRF 渲染出 2D 图像？</li>
<li>如何训练 NeRF？</li>
</ul>
<h1 id="重点内容"><a class="markdownIt-Anchor" href="#重点内容">#</a> 重点内容</h1>
<h2 id="abstract-摘要"><a class="markdownIt-Anchor" href="#abstract-摘要">#</a> Abstract 摘要</h2>
<h3 id="原文对照"><a class="markdownIt-Anchor" href="#原文对照">#</a> 原文对照</h3>
<blockquote>
<p><strong>Abstract.</strong></p>
<p>We present a method that achieves state-of-the-art results for <strong>synthesizing novel views of complex scenes</strong> by <strong>optimizing an underlying continuous volumetric scene function</strong> <em>using a sparse set of input views</em>.</p>
<p>我们提出了一种，通过<em>使用稀疏的输入视图集</em>来<strong>优化底层的连续体积场景函数</strong>，从而获得用于<strong>合成复杂场景的新视图</strong>的最先进的结果。</p>
<p>Our algorithm represents a scene using <em>a fully-connected (nonconvolutional) deep network</em>, whose input is a single continuous 5D coordinate (spatial location (<em>x, y, z</em>) and viewing direction (<em>θ, φ</em>)) and whose output is <strong>the volume density and view-dependent emitted radiance at that spatial location</strong>.</p>
<p>我们的算法使用<em>完全连通 (非卷积) 的深度网络</em>来表示场景，其输入是<strong>单个连续的 5D 坐标 (空间位置 (x，y，z) 和观察方向 (θ，φ))</strong>，其<strong>输出是该空间位置的体积密度和依赖于视图的发射辐射</strong>。</p>
<p>We synthesize views by querying 5D coordinates along camera rays and use <strong>classic volume rendering techniques</strong> to project the output <strong>colors and densities</strong> into an image.</p>
<p>我们通过查询相机光线上的 5D 坐标来合成视图，并使用经典的<strong>体绘制技术</strong>将输出的<strong>颜色和密度</strong>投影到图像中。</p>
<p>Because volume rendering is naturally differentiable, the only input required to optimize our representation is <strong>a set of images with known camera poses</strong>.</p>
<p>由于体积渲染是自然可分化的，因此唯一需要优化的输入表示 是一组<strong>具有已知相机姿态的图像</strong>。</p>
<p>We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.</p>
<p>我们描述了如何有效地优化神经辐射场，以渲染具有复杂几何形状和外观的场景的光逼真新颖视图，并展示了优于先前神经渲染和视图合成工作的结果。</p>
<p>View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</p>
<p>视图合成结果最好以视频的形式观看，所以我们力荐读者们观看我们为了证明对比所外加的视频。</p>
</blockquote>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结">#</a> 总结</h3>
<ol>
<li>
<p>本文的主要工作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">提出了一种方法：合成复杂场景的新视图</span><br><span class="line">神经网络MLP：全连接的非卷积深度网络</span><br><span class="line">	输入：一组具有已知【相机姿态】的图像（【相机姿态】：空间位置、观察方向）</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>重点问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用神经辐射场来表示场景</span><br><span class="line">如何从NeRF渲染出图片？——基于辐射场的体素渲染算法</span><br><span class="line">NERF的训练过程</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="重点问题讨论"><a class="markdownIt-Anchor" href="#重点问题讨论">#</a> 重点问题讨论</h1>
<h2 id="用神经辐射场来表示场景"><a class="markdownIt-Anchor" href="#用神经辐射场来表示场景">#</a> 用神经辐射场来表示场景</h2>
<p><img data-src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20220318153013619.png" alt="image-20220318153013619"></p>
<p>NeRF 函数是将一个连续的场景表示为一个输入为 5D 向量的函数，包括一个空间点的 3D 坐标位置 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D+%3D+%28x%2Cy%2Cz%29" alt="[公式]"> ，以及视角方向 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bd%7D+%3D+%28%5Ctheta%2C+%5Cphi%29" alt="[公式]"> 。这个神经网络可以写作：</p>
<p><img data-src="https://www.zhihu.com/equation?tex=F_%7B%5CTheta+%7D%3A%28%5Cbm%7Bx%7D%2C+%5Cbm%7Bd%7D%29+%5Crightarrow%28%5Cbm%7Bc%7D%2C+%5Csigma%29" alt="[公式]"></p>
<p>输出结果中， <img data-src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> 是对应 3D 位置（或者说是体素）的密度，而 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bc%7D+%3D+%28r%2Cg%2Cb%29" alt="[公式]"> 是视角相关的该 3D 点颜色。在具体的实现中， <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="[公式]"> 首先输入到 MLP 网络中，并输出 <img data-src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> 和中间特征，中间特征和 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bd%7D" alt="[公式]"> 再输入到额外的全连接层中并预测颜色。因此，体素密度只和空间位置有关，而颜色则与空间位置以及观察的视角都有关系。基于 view dependent 的颜色预测，能够得到不同视角下不同的光照效果，非常惊艳。可以看出，NeRF 函数的表示非常简单。</p>
<h2 id="如何从nerf渲染出图片基于辐射场的体素渲染算法"><a class="markdownIt-Anchor" href="#如何从nerf渲染出图片基于辐射场的体素渲染算法">#</a> 如何从 NeRF 渲染出图片？—— 基于辐射场的体素渲染算法</h2>
<p>NeRF 函数得到的是一个 3D 空间点的颜色和密度信息，但当用一个相机去对这个场景成像时，所得到的 2D 图像上的一个像素实际上对应了一条从相机出发的射线上的所有连续空间点。我们需要通过渲染算法从这条射线上的所有点得到这条射线的最终渲染颜色。同时，为了保证网络可以训练，NeRF 中需要采用可微的渲染方法。</p>
<p>*<strong>1. 经典的 volume rendering 方式 *</strong></p>
<p>论文中首先介绍了经典的体素渲染 volume rendering [2] 方法。体素密度 <img data-src="https://www.zhihu.com/equation?tex=%5Csigma%28%5Cbm%7Bx%7D%29" alt="[公式]"> 可以被理解为，一条射线 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Br%7D" alt="[公式]"> 在经过 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="[公式]"> 处的一个无穷小的粒子时被终止的概率，这个概率是可微的。换句话说，有点类似于这个点的不透明度。由于一条射线上的点是连续的，自然的想法是这条射线的颜色可以由积分的方式得到。将一个相机射线标记为 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Br%7D%28t%29+%3D+%5Cbm%7Bo%7D+%2B+t%5Cbm%7Bd%7D" alt="[公式]"> ，这里 <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bo%7D" alt="[公式]"> 是射线原点， <img data-src="https://www.zhihu.com/equation?tex=%5Cbm%7Bd%7D" alt="[公式]"> 是前述的相机射线角度，t 的近段和远端边界分别为 <img data-src="https://www.zhihu.com/equation?tex=t_n" alt="[公式]"> 以及 <img data-src="https://www.zhihu.com/equation?tex=t_f" alt="[公式]"> 。那么这条射线的颜色，则可以用积分的方式表示为：</p>
<p><img data-src="https://www.zhihu.com/equation?tex=C%28r%29+%3D+%5Cint_%7Bt_n%7D%5E%7Bt_f%7D+T%28t%29%5Ccdot%5Csigma%28%5Cbm%7Br%7D%28t%29%29%5Ccdot%5Cbm%7Bc%7D%28%5Cbm%7Br%7D%28t%29%2C%5Cbm%7Bd%7D%29dt" alt="[公式]"></p>
<p>此处， <img data-src="https://www.zhihu.com/equation?tex=T%28t%29" alt="[公式]"> 是射线从 <img data-src="https://www.zhihu.com/equation?tex=t_n" alt="[公式]"> 到 <img data-src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 这一段路径上的累积透明度，可以被理解为这条射线从 <img data-src="https://www.zhihu.com/equation?tex=t_n" alt="[公式]"> 到 <img data-src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 一路上没有击中任何粒子的概率。其具体的形式为：</p>
<p><img data-src="https://www.zhihu.com/equation?tex=T%28t%29+%3D+exp%28-%5Cint_%7Bt_n%7D%5E%7Bt%7D%5Csigma%28%5Cbm%7Br%7D%28s%29%29ds%29" alt="[公式]"></p>
<p>但是，实际应用中，我们并不可能够用 NeRF 去估计连续的 3D 点的信息，因此就需要数值近似的方法。这里也是 NeRF 中非常核心的一个部分。</p>
<h2 id="nerf的训练过程"><a class="markdownIt-Anchor" href="#nerf的训练过程">#</a> NeRF 的训练过程</h2>
<p>介绍完 NeRF 的基本原理，论文接下来介绍了 NeRF 中的两个重要 Trick，以及训练方式。</p>
<p>*<strong>1. 训练高质量 NeRF 的重要技巧 —— 位置信息编码 *</strong></p>
<p>NeRF 函数的输入为位置和角度信息，作者发现直接将位置和角度作为网络的输入得到的结果是相对模糊的（见实验部分）。而用 positon encoding 的方式将位置信息映射到高频则能有效提升清晰度效果。具体而言，这里采用的是与 Transformer 中类似的正余弦周期函数的形式。我的理解是，采用 position encoding 能够使得网络更容易的理解并建模位置信息。</p>
<p>*<strong>2. 训练高质量 NeRF 的重要技巧 —— 多层级体素采样 *</strong></p>
<p>NeRF 的渲染过程计算量很大，每条射线都要采样很多点。但实际上，一条射线上的大部分区域都是空区域，或者是被遮挡的区域，对最终的颜色没有啥贡献。因此，作者采用了一种 “coarse to fine&quot; 的形式，同时优化 coarse 网络和 fine 网络。这一部分也非常有趣。首先对于 coarse 网络，我们可以采样较为稀疏的 <img data-src="https://www.zhihu.com/equation?tex=N_c" alt="[公式]"> 个点，并将前述的离散求和函数重新表示为：</p>
<p><img data-src="https://www.zhihu.com/equation?tex=%5Chat%7BC%7D_c%28%5Cbm%7Br%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Bw_i%5Cbm%7Bc%7D_i%7D" alt="[公式]"> ，其中 <img data-src="https://www.zhihu.com/equation?tex=w_i+%3D+T_i%5Ccdot%281-+exp%28-%5Csigma_i%5Ccdot%5Cdelta_i%29%29" alt="[公式]"></p>
<p>接下来，可以对 <img data-src="https://www.zhihu.com/equation?tex=w_i" alt="[公式]"> 做归一化</p>
<p><img data-src="https://www.zhihu.com/equation?tex=%5Chat%7Bw%7D_i+%3D+%5Cfrac%7Bw_i%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN_c%7Dw_j%7D" alt="[公式]"></p>
<p>此处的 <img data-src="https://www.zhihu.com/equation?tex=%5Chat%7Bw_i%7D" alt="[公式]"> 可以看作是沿着射线的概率密度函数（PDF），如下图所示。通过这个概率密度函数，我们可以粗略地得到射线上物体的分布情况。</p>
<p>接下来，基于得到的概率密度函数来采样 <img data-src="https://www.zhihu.com/equation?tex=N_f" alt="[公式]"> 个点，并用这<img data-src="https://www.zhihu.com/equation?tex=N_f" alt="[公式]"> 个点和前面的<img data-src="https://www.zhihu.com/equation?tex=N_c" alt="[公式]"> 个点一同计算 fine 网络的渲染结果 <img data-src="https://www.zhihu.com/equation?tex=%5Chat%7BC%7D_f%28%5Cbm%7Br%7D%29" alt="[公式]"> 。虽然 coarse to fine 是计算机视觉领域中常见的一个思路，但这篇方法中用 coarse 网络来生成概率密度函数，再基于概率密度函数采样更精细的点算的上是很有趣新颖的做法了。二阶段的采样示意图如下所示：</p>
<p>*<strong>3. 损失函数与一些训练细节 *</strong></p>
<p>最后，训练损失的定义倒是非常简单，直接定义在渲染结果上的 L2 损失 (同时优化 coarse 和 fine）：</p>
<p><img data-src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+%5Csum_%7Br%5Cin+R%7D%5Cleft%5B+%5Cleft+%5C%7C+%5Chat%7BC%7D_c%28r%29+-+C%28r%29+%5Cright+%5C%7C_2%5E2+++-+%5Cleft+%5C%7C+%5Chat%7BC%7D_f%28r%29+-+C%28r%29+%5Cright+%5C%7C_2%5E2+%5Cright%5D" alt="[公式]"></p>
<p>训练时长方面，论文中提及的速度是一个场景要用单卡 V100 训练 1-2 天左右。</p>

  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">更新于</span>
    <time title="修改时间：2022-03-20 00:35:57" itemprop="dateModified" datetime="2022-03-20T00:35:57+08:00">2022-03-20</time>
  </span>
</div>

      
<div class="reward">
  <button><i class="ic i-heartbeat"></i> 赞赏</button>
  <p>请我喝[奶茶]~(￣▽￣)~*</p>
  <div id="qr">
      
      <div>
        <img data-src="/images/wechat.png" alt="ayanamisaki 微信支付">
        <p>微信支付</p>
      </div>
      
      <div>
        <img data-src="/images/alipay.png" alt="ayanamisaki 支付宝">
        <p>支付宝</p>
      </div>
  </div>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>本文作者： </strong>ayanamisaki <i class="ic i-at"><em>@</em></i>૮₍ ˃ ⤙ ˂ ₎ა 
  </li>
  <li class="link">
    <strong>本文链接：</strong>
    <a href="https://ayanamisaki.github.io/2022/03/18/NeRF%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-NeRF%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields%20for%20View%20Synthesis/" title="NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis">https://ayanamisaki.github.io/2022/03/18/NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis/</a>
  </li>
  <li class="license">
    <strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/2022/03/18/Markdown%E4%BD%BF%E7%94%A8%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclil3m4ej20zk0m8tn8.jpg" title="Markdown使用简易教程">
  <span class="type">上一篇</span>
  <span class="category"><i class="ic i-flag"></i> </span>
  <h3>Markdown使用简易教程</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="/2022/03/19/%E5%B0%8F%E6%9C%88%E9%A5%BC%E6%97%A5%E8%AE%B02%EF%BC%9A%E4%B8%8B%E9%9B%AA%E4%BA%86/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gicitht3xtj20zk0m8k5v.jpg" title="小月饼日记2：下雪了">
  <span class="type">下一篇</span>
  <span class="category"><i class="ic i-flag"></i> </span>
  <h3>小月饼日记2：下雪了</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="文章目录">
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text"> 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.1.</span> <span class="toc-text"> 基本信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83"><span class="toc-number">1.2.</span> <span class="toc-text"> 核心</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E9%87%8D%E7%82%B9"><span class="toc-number">1.3.</span> <span class="toc-text"> 理解重点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%E5%86%85%E5%AE%B9"><span class="toc-number">2.</span> <span class="toc-text"> 重点内容</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract-%E6%91%98%E8%A6%81"><span class="toc-number">2.1.</span> <span class="toc-text"> Abstract 摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E6%96%87%E5%AF%B9%E7%85%A7"><span class="toc-number">2.1.1.</span> <span class="toc-text"> 原文对照</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.1.2.</span> <span class="toc-text"> 总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%E9%97%AE%E9%A2%98%E8%AE%A8%E8%AE%BA"><span class="toc-number">3.</span> <span class="toc-text"> 重点问题讨论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E7%A5%9E%E7%BB%8F%E8%BE%90%E5%B0%84%E5%9C%BA%E6%9D%A5%E8%A1%A8%E7%A4%BA%E5%9C%BA%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text"> 用神经辐射场来表示场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BB%8Enerf%E6%B8%B2%E6%9F%93%E5%87%BA%E5%9B%BE%E7%89%87%E5%9F%BA%E4%BA%8E%E8%BE%90%E5%B0%84%E5%9C%BA%E7%9A%84%E4%BD%93%E7%B4%A0%E6%B8%B2%E6%9F%93%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text"> 如何从 NeRF 渲染出图片？—— 基于辐射场的体素渲染算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nerf%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text"> NeRF 的训练过程</span></a></li></ol></li></ol>
      </div>
      <div class="related panel pjax" data-title="系列文章">
      </div>
      <div class="overview panel" data-title="站点概览">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="ayanamisaki"
      data-src="/images/avatar.jpg">
  <p class="name" itemprop="name">ayanamisaki</p>
  <div class="description" itemprop="description"></div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">8</span>
        <span class="name">文章</span>
      </a>
    </div>
</nav>

<div class="social">
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>首页</a>
  </li>

    
  <li class="item">
    <a href="/about/" rel="section"><i class="ic i-user"></i>关于</a>
  </li>

        
  <li class="item dropdown">
      <a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a>
    <ul class="submenu">

        
  <li class="item">
    <a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a>
  </li>

        
  <li class="item">
    <a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a>
  </li>

        
  <li class="item">
    <a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a>
  </li>

  </ul>
    
  <li class="item">
    <a href="/friends/" rel="section"><i class="ic i-heart"></i>朋友圈</a>
  </li>

    
  <li class="item">
    <a href="/links/" rel="section"><i class="ic i-magic"></i>links</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="/2022/03/18/Markdown%E4%BD%BF%E7%94%A8%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="/2022/03/19/%E5%B0%8F%E6%9C%88%E9%A5%BC%E6%97%A5%E8%AE%B02%EF%BC%9A%E4%B8%8B%E9%9B%AA%E4%BA%86/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>随机文章</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/19/%E5%B0%8F%E6%9C%88%E9%A5%BC%E6%97%A5%E8%AE%B03%EF%BC%9A%E5%B0%8F%E7%8C%AB%E6%AF%8F%E5%A4%A9%E8%A6%81%E7%9D%A1%E6%BB%A120%E5%B0%8F%E6%97%B6/" title="小月饼日记3:小猫每天要睡满20小时">小月饼日记3:小猫每天要睡满20小时</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/18/%E5%B0%8F%E6%9C%88%E9%A5%BC%E6%97%A5%E8%AE%B01%E4%BB%86%E4%BA%BA%E9%83%BD%E6%98%AF%E7%AC%A8%E8%9B%8B/" title="小月饼日记1:仆人都是笨蛋">小月饼日记1:仆人都是笨蛋</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/20/%E8%BF%99%E6%AC%A1%E7%9C%9F%E7%9A%84%E5%AD%A6%E4%BC%9A%E4%BA%86%E9%80%92%E5%BD%92/" title="这次真的学会了递归">这次真的学会了递归</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/18/Markdown%E4%BD%BF%E7%94%A8%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/" title="Markdown使用简易教程">Markdown使用简易教程</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/19/%E5%B0%8F%E6%9C%88%E9%A5%BC%E6%97%A5%E8%AE%B02%EF%BC%9A%E4%B8%8B%E9%9B%AA%E4%BA%86/" title="小月饼日记2：下雪了">小月饼日记2：下雪了</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/20/%E5%B0%8F%E6%9C%88%E9%A5%BC%E6%97%A5%E8%AE%B04%EF%BC%9A%E6%82%A0%E9%97%B2%E7%9A%84%E4%B8%80%E5%A4%A9/" title="小月饼日记4:悠闲的一天">小月饼日记4:悠闲的一天</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/17/hello-world/" title="初めまして！">初めまして！</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/03/18/NeRF%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-NeRF%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields%20for%20View%20Synthesis/" title="NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis">NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>最新评论</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2022</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">ayanamisaki @ ayanamisaki</span>
  </div>
  <div class="powered-by">
    基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2022/03/18/NeRF论文笔记1-NeRF Representing Scenes as Neural Radiance Fields for View Synthesis/',
    favicon: {
      show: "（●´3｀●）やれやれだぜ",
      hide: "(´Д｀)大変だ！"
    },
    search : {
      placeholder: "文章搜索",
      empty: "关于 「 ${query} 」，什么也没搜到",
      stats: "${time} ms 内找到 ${hits} 条结果"
    },
    valine: true,fancybox: true,
    copyright: '复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




</body>
</html>
